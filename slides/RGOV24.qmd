---
title: "Wrangling Data with DuckDB"
author: "Will Angel"
date: "2024-10-29"
location: "RGOV24"
format:
  revealjs: 
    theme: blood
incremental: true

editor: visual
---


## Key Takeaways
1. DuckDB is a *very* fast in-process database.
2. Duckplyr is a package for using Dplyer on DuckDB
3. You can use Duckplyr as a *drop in replacement to get a 10-20x speedup for dplyr code on large datasets*

## Agenda
* What is DuckDB
* Why should you care about DuckDB
* When should you use DuckDB
* How can you use DuckDB
* What is Duckplyr
* Data processing performance and profiling


# DuckDB?

## What is DuckDB? {.smaller}

DuckDB is an open source fast in-process analytical database!

* Open Source: Free & Open
* Fast: Performant. **Quickly and efficiently runs analytical SQL queries**. Speed means cheaper if you're doing cloud processing.
* In-process: **Runs locally** without a server, like SQLite.
* Analytical: DuckDB is optimized for **fast aggregations** and analytical queries to support online analytical processing (OLAP). DuckDB supports ACID transactions, but is not as fast for online transaction processing (OLTP) workloads.
* Database: DuckDB can be used to efficiently store relational data.


## Why you should care!
* DuckDB is a great tool for local SQL analysis. Import a file and you can do SQL locally!
* DuckDB is starting to power the next generation of embedding analytical tools, so expect browser based data filtering tools to get more powerful. 

## Why you should care (more technical)
* DuckDB is like SQLite but for data processing. You can crunch significant amounts of data locally without spinning up a full database / data warehouse server, which may create significant time/cost savings and simplify system design.
* DuckDB is versatile and fast for data processing. Competitive with spark/polars in benchmarks
* DuckDB is portable with zero dependencies.

##  When should you use DuckDB
* You should use DuckDB for data processing!
* You have medium largish data
* You don't want the hassle of procuring larger computing resources

##  How can you use DuckDB
* Directly use the DuckDB package.
* use DBplyr to connect to a local DuckDB database
* use Duckplyr!


## What is Duckplyr
* Duckplyr is a drop in replacement for Dplyr!
* Duckplyr uses DuckDB’s “relational” API to skip the SQL and directly construct logical query plans.
* This means you can speed up your Dplyr code by ~10-20x by simply changing the package! 
* Out of memory processing!
* Unsupported operations will fall back to Dplyr, so your code will always run!

```
install.packages("duckplyr")
```

##  Data processing performance and profiling

* DuckDB:
	* versus dplyr
	* versus dbplyr
	* versus data.frame
	* versus tibbles
	* versus data.table
	* versus polars

## Dplyr versus Duckplyr

To compare Dplyr and Duckplyr performance, we'll look at the Global Lake area, Climate, and Population Dataset (GLCP). This includes almost 80 million records of temperature data for lakes, for a 3.9GB Csv file. 

Dplyr:
```
data %>%
  distinct(HYBAS_ID) %>%
  nrow()
```

Duckplyr:
```
data %>%
  duckplyr::as_duckplyr_tibble() %>%
  distinct(HYBAS_ID) %>%
  nrow()
```





## dply

```
benchmark_result <- microbenchmark(
  dplyr = {
    data %>%
      distinct(HYBAS_ID) %>%
      nrow()
  },
  duckdb = {
    dbGetQuery(con, "SELECT COUNT(DISTINCT HYBAS_ID) FROM data")[[1]]
  },
  duckplyr = {
    data %>%
      duckplyr::as_duckplyr_tibble() %>%
      distinct(HYBAS_ID) %>%
      nrow()
  },
  times = trials
)
```

---





